# -*- coding: utf-8 -*-
"""Cutomer_Reurn_Probablity.ipynb

Automatically generated by Colaboratory.


In this project, we are trying to predict the probablity of customers returning to the store using given data

Importing libraries required to buid the model
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import datetime as dt
import seaborn as sns


from sklearn.cluster import KMeans
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold, cross_val_score, train_test_split
import sklearn.metrics as metrics
from sklearn.metrics import accuracy_score

# Reading the data set
df = pd.read_csv("shop_data.csv")

df.head()

# Checking the number of unique values 
len(df['id'].unique()),len(df)

"""## Data Cleaning"""

# Checking if there are any duplicate records
df.duplicated().sum()

# FInding the percentage of missing values for all columns

percent_missing = df.isnull().sum()/ len(df) * 100
percent_missing

# Checking the distribution of null values in the 'brand' column

null_brand = pd.isnull(df.brand)
null_brand = df[null_brand]
null_brand.describe()

"""We can see that brands belonging to a single chain 14 are missing. We can conclude this because std = 0 for chain column above"""

# We will replace the null values with a single value = 10
# We are asssuming that these products belong to a single brand i.e. brand = 10

df.brand = df["brand"].fillna(10)

percent_missing = df.isna().sum()/ len(df) * 100
percent_missing

"""Now we can see that percentage of missing values in brand column has reduced to 0.
However, there are still some values in other columns that are null.
Since these records are relative small in number (lees than 2%), we can delete these records
"""

# Deleting the records with null values

df = df.dropna()
percent_missing = df.isnull().sum()/ len(df) * 100
percent_missing

df.shape

"""## Splitting the dataset in two parts to find the target variable

Since we don't have a target variable for predicting probablity, we will have to engineer a variable that we aim to predict

We have records from 03/02/2012 to 07/23/2013. 
We are splitting the dataset into two parts 1) with records before 04/23/2013 and 2) with records after 04/23/2013.

Using these two subsets, we will find the last day of purchase before split date and first day of purchase after split date. This will provide us with the next purchase day of customers with respect to a refernce date (04/23/2013). We are assuming that the higher this value, the less probable is the customer to return.
"""

# Data before 04/23/2013

df_below = df[df['date'] < '2013-04-23']

# Data after 04/23/2013

df_after = df[df['date'] > '2013-04-23']

# Finding the last purchase day before 04/23/2013 for all customers

last = df_below.groupby('id')['date'].agg('max').reset_index()

last

# Finding the first purchase day after 04/23/2013 for all customers

next = df_after.groupby('id')['date'].agg('min').reset_index()

next

last_next = pd.merge (last,next, on = "id", how = "left")

last_next

last_next[['date_x',"date_y"]] = last_next[['date_x',"date_y"]].apply(pd.to_datetime)

last_next.dtypes

last_next['day_diff'] = (last_next['date_y'] - last_next['date_x']).dt.days

last_next

df_new = pd.merge(df_below,last_next, on = "id", how = "left")

df_new.drop(['date_x','date_y'],axis = 1,inplace = True)

"""We can see that some of the 'day_diff' values are Null. This is because these customers do not have any purchases after 04/23/2013. Thus, we will replace them with 999 assuming they are highly unlikely to return to the store."""

df_new['day_diff'] = df_new['day_diff'].fillna(999)

df_new.shape

# Checking for null values

df_new['day_diff'].isna().sum()

# Checking the distribution of our target variable

df_new['day_diff'].value_counts()

# Converting target variable to integer

df_new['day_diff'] = df_new['day_diff'].astype(int)

"""# Feature Engineering using RFM Analysis

## Recency Calculation
"""

# Making dataset to store last purchase dates of each cutomer before the refernce date

df_last_purchase = df_new.groupby('id').date.max().reset_index()

# Calculating the last purchase date

df_last_purchase.columns = ['id','MaxPurchaseDate']
df_last_purchase.MaxPurchaseDate = pd.to_datetime(df_last_purchase.MaxPurchaseDate)

# Calculating how recently the customer visisted the store before the refernce date

df_last_purchase['recency'] = (df_last_purchase['MaxPurchaseDate'].max() - df_last_purchase['MaxPurchaseDate']).dt.days

# Merging dataset with our main dataset

df_new = pd.merge(df_new, df_last_purchase[['id', 'recency']], on='id')

df_new

# We will be applying K-means clustering to cluster the recency values together

sse={}
recency = df_new[['recency']]
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(recency)
    recency["clusters"] = kmeans.labels_
    sse[k] = kmeans.inertia_

# Plotting the elbow curve of k values on X axis and error values on Y axis

plt.figure()
plt.plot(list(sse.keys()), list(sse.values()))
plt.xlabel("Number of cluster")
plt.show()

"""We can observe that a value of k=4 will give us the best results with least time. Thus, we find recency cluster values with k=4."""

# Finding the cluster values and storing it in a variable 'Clusters'

kmeans = KMeans(n_clusters = 4)
kmeans.fit(df_new[['day_diff']])
df_new['Clusters'] = kmeans.predict(df_new[['day_diff']])

df_clusters = df_new['Clusters'].reset_index()

df_cluster = df_new[["Clusters","day_diff"]]

# Analyzing the cluster values to see their distribution

df_cluster.groupby('Clusters').describe().unstack()

df_new

"""## Frequency Calculation

We will be finding the frequency of visit for these customers
"""

df_quant = df_new[['id', 'date']]

# Dropping any duplicate date values

df_quant = df_quant.drop_duplicates(subset=['id','date'],keep='first')

# Grouping dataset by ids

df_quant= df_quant.groupby('id').date.count().reset_index()

df_quant.head()

# Renaming the date column as frequency

df_quant.columns = ['id', 'frequency']

df_quant.head()

# Using k-means clustering to find number of optimal clusters for frequency values 

sse={}
quant = df_quant[['frequency']]
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(quant)
    df_quant["freq_clusters"] = kmeans.labels_
    sse[k] = kmeans.inertia_

# Plotting the elbow curve for different k values against error values

plt.figure()
plt.plot(list(sse.keys()), list(sse.values()))
plt.xlabel("Number of cluster")
plt.show()

"""We can observe that k=5 will provide us with optimal results"""

# Calculating cluster values with k=4

kmeans = KMeans(n_clusters=5, max_iter=1000).fit(quant)
df_quant["freq_clusters"] = kmeans.labels_

df_quant

# Merging the data with main dataset

df_new = pd.merge(df_new,df_quant,how="left",on = "id")

df_new.head()

"""## Monetary Calculation

We will be finding the total amount for goods purchase by each customer
"""

df_mon = pd.DataFrame(df_new.groupby("id")['purchaseamount'].agg("sum"))

df_mon = df_mon.reset_index()

df_mon

# Using k-means clustering to find number of optimal clusters for monetary values 

sse={}
mon = df_mon[['purchaseamount']]
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(mon)
    df_mon["mon_cluster"] = kmeans.labels_
    sse[k] = kmeans.inertia_

# Plotting the elbow curve for different k values against error values

plt.figure()
plt.plot(list(sse.keys()), list(sse.values()))
plt.xlabel("Number of cluster")
plt.show()

# Calculating cluster values with k=5

kmeans = KMeans(n_clusters=5, max_iter=1000).fit(mon)
df_mon["mon_cluster"] = kmeans.labels_

df_mon

# Merging with main dataset

df_new = pd.merge(df_new,df_mon,how = "left",on = "id")

df_new

# Renaming columns for better understanding

df_new.rename(columns = {"purchaseamount_x":"purchaseamount","Clusters":"rec_cluster", "purchaseamount_y":"totalpurchaseamount"},inplace= True)

df_new = df_new.drop('Unnamed: 0', axis=1)

df_new.rename(columns={'day_diff':'NextPurchaseDay'}, inplace=True)

"""## Creating Target variable and Feature Set"""

# We will be assigning probablity ranges to customers based on the value of their Next Purchase Day
# The probablity ranges will be inversely proportional to the Next Purchase Day i.e. higher the Next purchase day, lower is the probablity of that customer coming back

df_new['ProbablityScore'] = '90-100'
df_new.loc[df_new.NextPurchaseDay > 10, 'ProbablityScore'] = '80-90'
df_new.loc[df_new.NextPurchaseDay>20, 'ProbablityScore'] = '70-80'
df_new.loc[df_new.NextPurchaseDay>30, 'ProbablityScore'] = '60-70'
df_new.loc[df_new.NextPurchaseDay > 40, 'ProbablityScore'] = '50-60'
df_new.loc[df_new.NextPurchaseDay>50, 'ProbablityScore'] = '40-50'
df_new.loc[df_new.NextPurchaseDay>60, 'ProbablityScore'] = '30-40'
df_new.loc[df_new.NextPurchaseDay > 70, 'ProbablityScore'] = '20-30'
df_new.loc[df_new.NextPurchaseDay>80, 'ProbablityScore'] = '10-20'
df_new.loc[df_new.NextPurchaseDay>90, 'ProbablityScore'] = '0-10'

# Dropping the column as we don't need it anymore

df_new = df_new.drop('NextPurchaseDay', axis=1)

df_new

df_new.dtypes

# Dropping columns as their variance is already captured in the cluster columns

df_new.drop(['recency','frequency', 'totalpurchaseamount'], axis=1, inplace=True)

# Dropping columns as date and id won't affect our target variable

df_new.drop(['id', 'date'], axis=1, inplace=True)

# Changing the datatypes of these columns to make them categorical

df_new[['chain','dept', 'category', 'company', 'brand', 'productmeasure', 'rec_cluster', 'freq_clusters', 'mon_cluster']] = df_new[['chain','dept', 'category', 'company', 'brand', 'productmeasure', 'rec_cluster', 'freq_clusters', 'mon_cluster']].astype("object")

df_numeric = df_new.select_dtypes(include = ['int','float'])

df_categor = df_new.select_dtypes(exclude = ['int','float'])

# Changing the datatype of categorical variables to category

df_categor = df_categor.astype('category')

df_train_new = pd.concat([df_numeric,df_categor],axis=1)

df_train_new.dtypes

# We are performing One Hot Encoding for few of our categorical variables

df_train_new = pd.merge(df_train_new, pd.get_dummies(df_train_new[['productmeasure', 'rec_cluster', 'freq_clusters', 'mon_cluster']]), left_index=True, right_index=True)

# Dropping these columns as their variance is already captured through One Hot Encoding

df_train_new.drop(['productmeasure', 'rec_cluster', 'freq_clusters', 'mon_cluster'], axis=1, inplace=True)

# This will be our final dataset which we will use for training testing and validating our machine learning models

df_train_new

df_train_new.dtypes

"""## Building a Machine Learning Model"""

# Extracting the target variable in y

y = df_train_new.ProbablityScore

# Creating a X dataset with all the remaining features in it

X = df_train_new.drop('ProbablityScore', axis=1)

# Splitting the dataset into train (60%), test (20%) and validation (20%) datasets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=44)

"""We will be testing different algorithms like Linear Regression, Naive Bayes, Random Forest and KNN, and then select the algorithm with best accuracy using cross-validation"""

models = []
models.append(("LR",LogisticRegression()))
models.append(("NB",GaussianNB()))
models.append(("RF",RandomForestClassifier()))
models.append(("KNN",KNeighborsClassifier()))

# Training models with different algorithms and calculating their accuracy

for name,model in models:
    kfold = KFold(n_splits=2, random_state=101)
    cv_result = cross_val_score(model,X_train,y_train, cv = kfold,scoring = "accuracy")
    print(name, cv_result)

"""From the above result we can see that Random Forest algorithm provides the best accuracy of 94%. Thus we will test this model with our train, test and validation datasets.

### Hyperparameter Tuning
"""

# Training the model with train dataset
# We will be using a few parameters to tune our model
# n_estimators = 100 which is the number of trees for the random forest. Generally higher number of trees provide better results
# n_jobs = 1 is the number of jobs that run in parallel. We are keeping it 1 because dataset is not very big and training the model does not take a lot of time
# bootstrap = False which denotes if bootstrap sampling needs to be used. We will keep it as false since we don't want sampled dataset to be used. Instead we want the entire dataset to be used
# random_state = 42 is a random number using which data is sampled to build trees. We can give any random number

m1 = RandomForestClassifier(n_estimators = 100,n_jobs=1,bootstrap=False,random_state = 42)
m1.fit(X_train, y_train)

# Calculating results for validation dataset using the trained model

y_pred = m1.predict(X_val)

# Calculating accuracy for validation dataset

metrics.accuracy_score(y_pred, y_val)

# Testing the model on test dataset

y_pred = m1.predict(X_test)

# Calculating the accuracy for test dataset

metrics.accuracy_score(y_pred, y_test)

"""We can see that the accuracy of the model is almost 95% which is very good. The accuracy of the model increased by 0.5% after hyperparameter tuning. Thus we can conclude that this model will be highly effective in predicting the probality of customers returning to the store.